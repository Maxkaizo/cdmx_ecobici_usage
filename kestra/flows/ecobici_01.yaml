id: ecobici_01
namespace: dataeng.zoomcamp
description: This is a manual workflow to extract data from Ecobici and load it to BigQuery.

inputs:
  - id: year
    type: SELECT
    displayName: Select year
    values: ["2023", "2024", "2025"]
    defaults: "2025"
    allowCustomValue: true

  - id: month
    type: SELECT
    displayName: Select month
    values: ["01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"]
    defaults: "01"

variables:
  year_month: "{{inputs.year}}-{{inputs.month}}"
  filename: "ecobici_{{vars.year_month}}.csv"

tasks:

  - id: sync_namespace_files
    type: io.kestra.plugin.scripts.shell.Commands
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    commands:
      - /app/kestra namespace files update dataeng.zoomcamp /app/scripts . --server=http://localhost:8080 --user=admin@kestra.io:admin

  - id: get_catalog
    type: io.kestra.plugin.scripts.python.Commands
    containerImage: python:3.10-slim
    namespaceFiles:
      enabled: true
    beforeCommands:
      - pip install requests beautifulsoup4 pandas lxml
    commands:
      - python get_files_catalog.py
      - python get_station_catalog.py
    outputFiles:
      - catalog.csv
      - stations.csv

  - id: extract_file_url
    type: io.kestra.plugin.scripts.shell.Commands
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    commands:
      - |
        url=$(awk -F',' '$1 == "{{inputs.year}}" && $2 == "{{inputs.month}}" {print $3}' {{outputs.get_catalog.outputFiles["catalog.csv"]}})
      - echo "::{\"outputs\":{\"url\":\"${url}\"}}::"

  - id: http_download
    type: io.kestra.plugin.core.http.Download
    uri: "{{outputs.extract_file_url.vars.url}}"

  - id: upload_stations_to_gcs
    type: io.kestra.plugin.gcp.gcs.Upload
    from: "{{ outputs.get_catalog.outputFiles['stations.csv'] }}"
    to: "gs://{{ secret('GCP_BUCKET_NAME') }}/stations/stations.csv"

  - id: upload_to_gcs
    type: io.kestra.plugin.gcp.gcs.Upload
    from: "{{ outputs.http_download.uri }}"
    to: "gs://{{ secret('GCP_BUCKET_NAME') }}/raw/{{render(vars.filename)}}"

  - id: load_to_bigquery
    type: io.kestra.plugin.scripts.python.Commands
    namespaceFiles:
      enabled: true
    containerImage: python:3.10
    beforeCommands:
      - pip install dlt[bigquery] pandas gcsfs google-cloud-bigquery-storage
      - |
        cat <<EOF > /tmp/gcp_creds.json
        {{ secret("GCP_CREDS") }}
        EOF
      - mkdir -p .dlt
      - |
        cat <<EOF > .dlt/secrets.toml
        [destination.bigquery]
        location = "{{ secret('GCP_LOCATION') }}"
        EOF
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp_creds.json
      FILENAME: "{{render(vars.filename)}}"
    commands:
      - python extract_load_dlt.py

  - id: run_dbt
    type: io.kestra.plugin.scripts.python.Commands
    containerImage: maxkaizo/dbt-ecobici:latest
    beforeCommands:
      - |
        cat <<EOF > /tmp/gcp_creds.json
        {{ secret("GCP_CREDS") }}
        EOF
      - mkdir -p /root/.dbt
      - |
        cat <<EOF > /root/.dbt/profiles.yml
        ecobici_profile:
          target: dev
          outputs:
            dev:
              type: bigquery
              method: service-account
              project: {{ secret('GCP_PROJECT_ID') }}
              dataset: dataeng_448500_ecobici_ds
              location: {{ secret('GCP_LOCATION') }}
              threads: 1
              keyfile: /tmp/gcp_creds.json
        EOF
    commands:
      - dbt run --project-dir /app --profiles-dir /root/.dbt
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp_creds.json

  - id: purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: To avoid cluttering your storage, we will remove the downloaded files

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{ secret('GCP_CREDS') }}"
      projectId: "{{ secret('GCP_PROJECT_ID') }}"
      location: "{{ secret('GCP_LOCATION') }}"
      bucket: "{{ secret('GCP_BUCKET_NAME') }}"
