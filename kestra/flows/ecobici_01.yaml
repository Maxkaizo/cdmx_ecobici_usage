id: ecobici_01
namespace: dataeng.zoomcamp
description: This is a workflow that extract data from ecobici's portal and loads it to a bigquery db

# Input selection
inputs:
  - id: year
    type: SELECT
    displayName: Select year
    values: ["2023", "2024", "2025"]
    defaults: "2025"
    allowCustomValue: true # allows you to type 2021 from the UI for the homework ðŸ¤—

  - id: month
    type: SELECT
    displayName: Select month
    values: ["01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"]
    defaults: "01"

# Variables definition
variables:
  year_month: "{{inputs.year}}-{{inputs.month}}"
  filename: "ecobici_{{vars.year_month}}.csv"

tasks:
- id: get_catalog # download the url list and station's information from ecobici's portal 
  type: io.kestra.plugin.scripts.python.Commands
  namespaceFiles:
    enabled: true
  containerImage: python:3.10-slim
  beforeCommands:
    - pip install requests beautifulsoup4 pandas lxml
  commands:
    - python get_files_catalog.py
    - python get_station_catalog.py
  outputFiles:
    - catalog.csv
    - stations.csv

- id: extract_file_url # extract the url for the desired file from the url catalog
  type: io.kestra.plugin.scripts.shell.Commands
  taskRunner:
    type: io.kestra.plugin.core.runner.Process
  commands:
    - |
      url=$(awk -F',' '$1 == "{{inputs.year}}" && $2 == "{{inputs.month}}" {print $3}' {{outputs.get_catalog.outputFiles["catalog.csv"]}})
    - echo "::{\"outputs\":{\"url\":\"${url}\"}}::"

- id: http_download # download the desired file
  type: io.kestra.plugin.core.http.Download
  uri: "{{outputs.extract_file_url.vars.url}}"

- id: upload_stations_to_gcs
  type: io.kestra.plugin.gcp.gcs.Upload
  from: "{{ outputs.get_catalog.outputFiles['stations.csv'] }}"
  to: "gs://dataeng-448500-ecobici-bucket/stations/stations.csv"

- id: upload_to_gcs
  type: io.kestra.plugin.gcp.gcs.Upload
  from: "{{ outputs.http_download.uri }}"
  to: "gs://dataeng-448500-ecobici-bucket/raw/{{render(vars.filename)}}"

- id: load_to_bigquery
  type: io.kestra.plugin.scripts.python.Commands
  namespaceFiles:
    enabled: true
  containerImage: python:3.10
  beforeCommands:
    - pip install dlt[bigquery] pandas gcsfs google-cloud-bigquery-storage
    - |
      cat <<EOF > /tmp/gcp_creds.json
      {{ kv("GOOGLE_APPLICATION_CREDENTIALS") }}
      EOF
    - mkdir -p .dlt
    - |
      cat <<EOF > .dlt/secrets.toml
      [destination.bigquery]
      location = "us-central1"
      EOF
  env:
    GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp_creds.json
    FILENAME: "{{render(vars.filename)}}"
  commands:
    - python extract_load_dlt.py

- id: run_dbt
  type: io.kestra.plugin.scripts.python.Commands
  containerImage: maxkaizo/dbt-ecobici:latest
  namespaceFiles:
    enabled: true
  beforeCommands:
    - |
      cat <<EOF > /tmp/gcp_creds.json
      {{ kv("GOOGLE_APPLICATION_CREDENTIALS") }}
      EOF
    - mkdir -p /root/.dbt
    - |
      cat <<EOF > /root/.dbt/profiles.yml
      ecobici_profile:
        target: dev
        outputs:
          dev:
            type: bigquery
            method: service-account
            project: dataeng-448500
            dataset: dataeng_448500_ecobici_ds
            location: us-central1
            threads: 1
            keyfile: /tmp/gcp_creds.json
      EOF
  commands:
    - dbt run --project-dir /app --profiles-dir /root/.dbt


- id: purge_files
  type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
  description: To avoid cluttering your storage, we will remove the downloaded files

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDS')}}"
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_NAME')}}"
